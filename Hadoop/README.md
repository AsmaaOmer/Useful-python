## Hadoop
* Distributed file system run on commodity hardware. Unlike other distributed systems, HDFS is highly fault tolerant and designed using low-cost hardware. The core of Hadoop consists of a storage part, known as Hadoop Distributed File System (`HDFS`), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel.

## HIPI - Hadoop image processing interface
* http://hipi.cs.virginia.edu/
