## Hadoop
* Distributed file system run on commodity hardware. Unlike other distributed systems, HDFS is highly fault tolerant and designed using low-cost hardware. The core of Hadoop consists of a storage part, known as Hadoop Distributed File System (`HDFS`), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel.

## HIPI - Hadoop image processing interface
* http://hipi.cs.virginia.edu/

##Â References
* [LARGE SCALE SATELLITE IMAGE PROCESSING
USING HADOOP DISTRIBUTED SYSTEM](http://ijarcet.org/wp-content/uploads/IJARCET-VOL-3-ISSUE-3-731-735.pdf)
